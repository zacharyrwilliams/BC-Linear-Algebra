\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\colorlet{shadecolor}{orange!15}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{note}{Note}

\begin{document}
	\setcounter{section}{0}
	\title{Chapter 1: Matrices}
	
	\thispagestyle{empty}
	
	\begin{center}
		{\LARGE \bf Chapter 1}\\
	\end{center}

	\section{Matrices}
	\subsection{Basic Concepts}
	
	A \textit{matrix} is a rectangular array arranged in vertical columns.
	
	The matrix $L =
	\begin{bmatrix}
	1 & 3 \\
	5 & 2 \\
	0 & -1 \\
	\end{bmatrix} $ is said to have \textit{order} $3$ x $2$.
	
	The entries of a matrix are called \textit{elements}. $l_{1 2}$ refers to the element in the first row and second column of the matrix $L$.
	
	In general, a matrix $A$ of order $p$x$n$ has the form $A = \begin{bmatrix}
	a_{11} & a_{12} & \dots & a_{1n} \\
	a_{21} & a_{22} & \dots & a_{2n|} \\
	\vdots & \vdots & \ddots & \vdots \\
	a_{p1} & a_{p2} & \dots & a_{pn}
	\end{bmatrix} $
	
	Any element having its row index equal to its column index is a \textit{diagonal element}.
	
	A matrix is \textit{square} if it has the same number of rows as columns. In a square matrix, the elements $a_{11}, a_{22}, a_{33}, \dots$ form the \textit{main} or \textit{principal} diagonal.
	
	The elements of a matrix need not be numbers; they can be functions or matrices themselves.
	
	A \textit{row matrix} is a matrix having a single row; a \textit{column matrix} is a matrix having a single column. The elements of such a matrix are commonly called its \textit{components}, and the number of components its \textit{dimension}. 
	
	The term \textit{n-tuple} refers to either a row matrix or a column matrix having dimension $n$. 
	
	Two matrices $A$ and $B$ are \textit{equal} if they have the same order and if their correspondng elements are equal.
	
	\textit{The sum of two matrices of the same order} is a matrix obtained by adding together corresponding elements of the original matrices. Addition is not defined for matrices of different orders.
	
	\begin{theorem}
		If matrices $A$, $B$, $C$ all have the same order, then 
		
		\begin{enumerate}[label=(\alph*)]
			\item the commutative law of addition holds; that is, $A + B = B + A$
			\item the associative law of addition holds; that is, $A + (B  + C) = (A + B) + C$
		\end{enumerate}
	\end{theorem}
	
	\begin{shaded}
		\begin{proof}
			\begin{enumerate}[label=(\alph*)]
				\item Let $A = [a_{ij}] \land B = [b_{ij}]$. Then
				\begin{align*}
				A + B = [a_{ij}] + [b_{ij}] && \text{by defs. of A, B} \\
				= [a_{ij}] + [b_{ij}] && \text{by def. of matrix addition} \\
				= [b_{ij} + a_{ij}] && \text{by commutative property of addition} \\
				= [b_{ij}] + [a_{ij}] && \text{by def. of matrix addition} \\
				= B + A
				\end{align*}
				
				\item Let $A = [a_{ij}], B = [b_{ij}],$ and $C = [c_{ij}]$. Then
				\begin{align*}
				A + (B + C) = [a_{ij}] + ([b_{ij}] + [c_{ij}]) \\
				= [a_{ij}] + [b_{ij} + c_{ij}] && \text{by def. of matrix addition} \\
				= [a_{ij} + (b_{ij} + c_{ij})] && \text{by def. of matrix addition} \\
				= [(a_{ij} + b_{ij}) + c_{ij}] && \text{by associative property of addition} \\
				= [(a_{ij} + b_{ij})] + [c_{ij}] && \text{by def. of matrix addition}
				= (A + B) + C
				\end{align*} 
			\end{enumerate}
		\end{proof}
	\end{shaded}

	We define the matrix $0$ to be a matrix consisting of only zero elements. When a zero matrix has the same order as another matrix $A$, we have the additional property $A + 0 = A$.
	
	Subtraction of matrices is defined analogously to addition. The difference $B - A$ of two matrices of the same order is the matrix obtained by subtracting from the elements of $A$ the corresponding elements of $B$.
	
	A matrix $A$ can always be added to itself, forming the sum $A + A$. We would like to write $A + A = 2A$.
	
	The right side of the equation is a number times a matrix, a product known as \textit{scalar multiplication}.
	
	If $A = [a_{ij}] $ is a $p$x$n$ matrix and if $\lambda$ is a real number, then $ \lambda A = [\lambda a_{ij}], (i=1,2,\dots,p; j=1,2,\dots,n).$
	
	\begin{theorem}
		If $A \land B$ are matrices of the same order and if $\lambda_1 \land \lambda_2$ denote scalars, then the following distributive laws hold: 
		
		\begin{enumerate}[label=(\alph*)]
			\item $\lambda_1 (A+B) = \lambda_1A + \lambda_1B.$
			\item $(\lambda_1 + \lambda_2)A = \lambda_1A + \lambda_2A$
			\item $(\lambda_1\lambda_2)A = \lambda_1(\lambda_2A)$
		\end{enumerate}
	\end{theorem}

	\begin{shaded}
		\begin{proof}
			\begin{enumerate}[label=(\alph*)]
				\item Let $A = [a_{ij}] \land B = [b_{ij}]$. Then
				\begin{align*}
					\lambda_1(A+B) = \lambda_1([a_{ij}] + [b_{ij}]) \\
					= \lambda_1[(a_{ij} + b_{ij})] && \text{def. of matrix addition} \\
					= [\lambda_1(a_{ij} + b_{ij})] && \text{def. of scalar multiplication} \\
					= [(\lambda_1a_{ij} + \lambda_1b_{ij})] && \text{distributive property of scalars} \\
					= [\lambda_1a_{ij}] + [\lambda_1b_{ij}] && \text{def. of matrix addition} \\
					= \lambda_1[a_{ij}] + \lambda_1[b_{ij}] && \text{def. of scalar multiplication}
					= \lambda_1A + \lambda_1B
				\end{align*}
				
				\item Let $A = [a_{ij}]$. Then
				\begin{align*}
					(\lambda_1+\lambda_2)A = (\lambda_1+\lambda_2)[a_{ij}]\\
					= [(\lambda_1+\lambda_2)a_{ij}] && \text{def. of scalar multiplication} \\
					= [\lambda_1a_{ij} + \lambda_2a_{ij}] && \text{distributive property of multiplication} \\
					= [\lambda_1a_{ij}] + [\lambda_2a_{ij}] && \text{def. of matrix addition} \\
					= \lambda_1[a_{ij}] + \lambda_2[a_{ij}] && \text{def. of scalar multiplication} \\
					= \lambda_1A + \lambda_2A
				\end{align*} 
				
				\item Let $A = [a_{ij}]$. Then 
				\begin{align*}
					(\lambda_1\lambda_2)A = (\lambda_1\lambda_2)[a_{ij}] \\
					= [(\lambda_1\lambda_2)a_{ij}] && \text{def. of scalar multiplication} \\
					= [\lambda_1(\lambda_2a_{ij})] && \text{associative property of multiplication} \\
					= \lambda_1[\lambda_2a_{ij}] && \text{def. of scalar multiplication} \\
					= \lambda_1(\lambda_2A)
				\end{align*}
			\end{enumerate}
		\end{proof}
	\end{shaded}

	\pagebreak
	\subsection{Matrix Multiplication}
		A single system of two linear equations in two unknowns is 
		\begin{align*}
			2x + 3y = 10 \\
			4x + 5y = 20
		\end{align*}
		
		Combining all the coefficients of the variables on the left of each equation into a \textit{coefficient matrix}, all the variables into a column matrix of variables; and the constants on the right of each equation into another column matrix, we separate the matrix system
		\begin{center}
			$\begin{bmatrix}
			2 & 3 \\ 4 & 5
			\end{bmatrix}
			\begin{bmatrix}
			x \\ y
			\end{bmatrix}
			= \begin{bmatrix}
			10 \\ 20
			\end{bmatrix}$
		\end{center}
		
		We want to define matrix multiplication so that 
		\begin{center}
			$\begin{bmatrix}
			2 & 3 \\ 4 & 5
			\end{bmatrix}
			\begin{bmatrix}
			x \\ y
			\end{bmatrix}
			=\begin{bmatrix}
			(2x + 3y) \\ (4x + 5y)
			\end{bmatrix}
			= \begin{bmatrix}
			10 \\ 20
			\end{bmatrix}$
		\end{center}
	
		We shall define the product $AB$ of two matrices $A$ and $B$ when the number of columns of $A$ is equal to the number of rows of $B$, and the result will be a matrix having the same number of rows as A and the same number of columns as B.
		
		When the product $AB$ is considered, $A$ is said to \textit{premultiply} $B$, while $B$ is said to \textit{postmultiply} $A$.
		
		To calculate the $i-j$ element of $AB$, when the multiplication is defined, multiply the elements in the $i$th row of $A$ by the corresponding elements in the $j$th column of $B$ and sum the results.
		
		Let $AB = C = [c_{ij}]$. Then $[c_{ij}] = a_{i1}b_{1j} + a_{i2}b_{2j} + \dots + a_{ir}b_{rj} = \sum\limits_{k=1}^{r} a_{ik}b_{kj}$.
		
		Although matrix multiplication is not commutative, some matrix products are. Also, matrices exist for which $AB = 0$ without either $A$ or $B$ being $0$. The cancellation law also does not apply. In general, $AB = AC$ does not imply $B = C$.	
		
		\begin{theorem}
			If $A, B, \land C$ have appropriate orders so that the following additions and multiplications are defined, then
			\begin{enumerate}[label=(\alph*)]
				\item $A(BC) = (AB) C$ \hfill (associative law of multiplication)
				\item $A(B+C) = AB + AC$ \hfill (left distributive law)
				\item $(B+C)A = BA + CA$ \hfill (right distributive law)
			\end{enumerate}
		\end{theorem}
		
		\begin{shaded}
			\begin{proof}
				\begin{enumerate}[label=(\alph*)]
					\item Let $A = [a_{ij}]$ be an $m$x$n$ matrix, $B = [b_{ij}]$ be an $n$x$p$ matrix, and $C=[c_{ij}]$ be a $p$x$q$ matrix. Then
					\begin{align*}
						(AB)C = ([a_{ij}][b_{jk}])[c_{}] \\
						= (\sum\limits_{k=1}^{r} a_{ik}b_{kj})[c_{}] \\
						= \sum\limits_{l=1}^{p}(\sum\limits_{k=1}^{r} a_{ik}b_{kj})c_{} \\
						= \sum\limits_{l}([ab]_{il}[c]_{}) \\
						= [ab]_{il}[c]_{lj} \\
						= (AB)C
					\end{align*}
					
					\begin{note}
						I got lost in the sea of subscripts, but labeled appropriately (and arbitrarily) the logic still holds.
					\end{note}
					
					\item Let $A = [a_{ij}] \land B = [b_{ij}] \land C = [c_{ij}]$, where $A$ is $m$x$n$, and $B$ and $C$ are $n$x$p$. Then
					\begin{align*}
						A(B+C) = [a_{ij}]([b_{ij}] + [c_{ij}]) \\
						= [a_{ij}][(b_{ij} + c_{ij})] \\
						= \sum\limits_{k}a_{ik}(b_{kj}+c_{kj}) \\
						= \sum\limits_{k}a_{ik}b_{kj} + a_{ik}c_{kj} \\
						= \sum\limits_{k}a_{ik}b_{kj} + \sum\limits_{k}a_{ik}c_{kj} \\
						= [a_{ij}][b_{ij}] + [a_{ij}][c_{ij}] \\
						= AB + AC
					\end{align*} 
					
					\item Let $A = [a_{ij}] \land B = [b_{ij}] \land C = [c_{ij}]$, where $A$ is $m$x$n$, and $B$ and $C$ are $n$x$p$. Then
					\begin{align*}
						(B+C)A = ([b_{ij}] + [c_{ij}])[a_{ij}] \\
						= \sum\limits_{k}(b_{ik} + c_{ik})a_{kj} \\
						= \sum\limits_{k}(b_{ik}a_{kj} + c_{ik})a_{kj} \\
						= \sum\limits_{k}(b_{ik}a_{kj} + \sum\limits_{k}c_{ik})a_{kj} \\
						= BA + BC
					\end{align*}
				\end{enumerate}
			\end{proof}
		\end{shaded}
	
	\pagebreak
	\subsection{Special Matrices}
		The transpose of a matrix $A$, denoted by $A^T$, is obtained by converting all the rows of $A$ into the columns of $A^T$ while preserving the ordering of the rows/columns. More formally, if $A = [a_{ij}]$ is an $n$x$p$ matrix, then the transpose of A, denoted by $A^T = [a^T_{ij}]$, is a $p$x$n$ matrix where $a^T_{ij} = a_{ji}$.
		
		\begin{theorem}
			The following properties are true for any scalar $\lambda$ and any matrices for which the indicated additions and multiplications are defined.
			\begin{enumerate}[label=(\alph*)]
				\item $(A^T)^T = A$
				\item $(\lambda A)^T = \lambda A^T$
				\item $(A+B)^T = A^T + B^T$
				\item $(AB)^T = B^T A^T$
			\end{enumerate}
		\end{theorem}
		
		\begin{shaded}
			\begin{proof}
				\begin{enumerate}[label=(\alph*)]
					\item Let $A$ be an $m$x$n$ matrix $A=[a_{ij}]$. Then 
					\begin{align*}
					(A^T)^T = ([a_{ij}]^T)^T \\
					= ([a_{ji}])^T \\
					= [a_{ij}]
					\end{align*}
					
					\item 
					\begin{align*}
						(\lambda A)^T = (\lambda [a_{ij}])^T \\
						= ([\lambda a_{ij}])^T \\
						= [\lambda a_{ji}] \\
						= \lambda [a_{ji}] \\
						= \lambda [a_{ij}]^T \\
						= \lambda A^T
					\end{align*}
					
					\item
						\begin{align*}
							(A+B)^T = ([a_{ij}]+[b_{ij}])^T = ([a_{ij}+b_{ij}])^T = [a_{ji}+b_{ji}] = [a_{ji}]+[b_{ji}] = A^T + B^T
						\end{align*}
						
					\item Note: I'm pretty sure the following is wrong.
						\begin{align*}
							(AB)^T = (\sum_{k}a_{ik}b_{kj})^T = \sum_{k}a_{jk}b_{ki} = \sum_{k}a_{kj}^Tb_{ik}^T = \sum_{k}b_{ik}^Ta_{kj}^T = (B^TA^T)
						\end{align*}
				\end{enumerate}
			\end{proof}
		\end{shaded}
	
	A matrix $A$ is \textit{symmetric} if it equals its own transpose; that is, if $A = A^T$. A matrix $A$ is \textit{skew-symmetric} if it equals the negative of its transpose; that is, if $A = -A^T$.
	
	A \textit{submatrix} of a matrix $A$ is a matrix obtained by removing any number of rows or columns from $A$. By removing no rows and no columns from $A$, it follows that $A$ is a submatrix of itself.
	
	A matrix is \textit{partitioned} if it is divided into submatrices by horizontal or vertical lines between rows and columns.
	
	A \textit{zero row} in a matrix is a row containing only zero elements, whereas a nonzero row is a row that contains at least one nonzero element.
	
	\begin{defn}
		A matrix is in \textit{row-reduced form} if it satisfies the following four conditions:
		\begin{enumerate}
			\item All zero rows appear below nonzero rows when both types are present in the matrix.
			\item The first nonzero element in any nonzero row is 1.
			\item All elements directly below (that is, in the same column but in succeeding rows from) the first nonzero elements of a nonzero row are zero.
			\item The first nonzero element of any nonzero row appears in a later column (further to the right) than the first nonzero element in any preceding row.
		\end{enumerate}
	\end{defn}
	
	A \textit{diagonal matrix} is a square matrix having only zeros as non-diagonal elements.
	
	An \textit{identity matrix}, denoted by $I$, is a diagonal matrix having all its diagonal elements equal to 1, and 0 otherwise.
	
	If $A$ and $I$ are square matrices of the same order, then $AI = IA = A$.
	
	A \textit{block diagonal matrix} $A$ is one that can be partitioned into the form
	
	$A = \begin{bmatrix}
	A_{1} & 0 & \dots & 0 \\
	0 & A_{2} & 0 \dots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0  & \dots &\dots & A_{k}
	\end{bmatrix} $, where $A_1, A_2, ..., A_k$ are square matrices.
	
	A matrix $A=[a_{ij}]$ is \textit{upper triangular} if $a_{ij} = 0$ $\forall i>j$; that is, if all elements below the main diagonal are zero.
	
	If $a_{ij} = 0$ $\forall i<j$; that is, if all elements above the main diagonal are zero, then $A$ is \textit{lower triangular}.
	
	\begin{theorem}
		The product of two lower (upper) triangular matrices of the same order is also lower (upper) triangular.
	\end{theorem}

	\begin{shaded*}
		\begin{proof}
			Let $A = [a_{ij}] \land B = [b_{ij}]$ both be $n$x$n$ lower triangular matrices. Then
			
			$(AB)_{ij} = \sum\limits_{k=1}^{n}a_{ik}b_{kj}$
			
			Since $a_{ik} = 0$ $\forall i<j$
			
			? \\
			
			Since $b_{kj} = 0$ $\forall k<j$
			
			?? \\
		\end{proof}
	\end{shaded*}
	\pagebreak
	
	\subsection{Linear Systems of Equations}
	
	A system of $m$-linear equations in $n$-variables $x_1, x_2, \dots, x_n$ has the general form
\end{document}

